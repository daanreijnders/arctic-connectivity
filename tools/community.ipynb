{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import networkx as nx\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cart\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lonlat_from_pset(pset, timedelta64=None):\n",
    "    \"\"\"\n",
    "    Extract latitude and longitude data from particleSet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pset : str\n",
    "        string with path to ``parcels.ParticleSet`` output file\n",
    "    timedelta64 : np.timedelta64\n",
    "        relative timestamp to load data from pset at, relative to start time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lonlat_init\n",
    "        np.array with initial longitude-latitude pairs\n",
    "    lonlat_final\n",
    "        np.array with final longitude-latitude pairs    \n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(pset)\n",
    "    lons = ds['lon'].data\n",
    "    lats = ds['lat'].data\n",
    "    ids = ds['traj'].data\n",
    "    times = ds['time'].data\n",
    "\n",
    "    if np.any(np.diff(times[:,0]).astype(bool)):\n",
    "        warnings.warn(\"Not all starting times are equal. Behaviour may not be as expected.\", Warning)\n",
    "    if timedelta64:\n",
    "        # Determine which trajectory idx to use for searchsorted, \n",
    "        # since it must contain timestamps in the last index.\n",
    "        firstFullTrajectoryIdx = np.searchsorted(~np.isnat(times[:, -1]), True)\n",
    "        # Find index at which trajectories shoudl be investigated\n",
    "        final_tidx = np.searchsorted(times[firstFullTrajectoryIdx,:], \n",
    "                                     times[firstFullTrajectoryIdx,0] + timedelta64)\n",
    "        if final_tidx == times.shape[1]:\n",
    "            warnings.warn(\"`final_tidx` lies outside of time window. Choosing last index instead\", Warning)\n",
    "            final_tidx = times.shape[1]-1\n",
    "    else:\n",
    "        final_tidx = times.shape[1]-1\n",
    "    lonlatInit = np.dstack((lons[:,0], lats[:,0]))\n",
    "    lonlatFinal = np.dstack((lons[:,final_tidx], lats[:, final_tidx]))\n",
    "    ds.close()\n",
    "    return lonlatInit, lonlatFinal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class particles:\n",
    "    \"\"\"\n",
    "    Basic instance of particles object has lists holding the latitudes and longitudes of its points.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    lats : np.array\n",
    "        list of latitudes (in degrees)\n",
    "    lons : np.array\n",
    "        list of longitudes (in degrees)\n",
    "    lonlat : np.ndarray\n",
    "        2D array holding pairs of latitude and longitude of each particle\n",
    "    n : int\n",
    "        number of gridpoints\n",
    "    idx : np.ndarray\n",
    "        index of each gridpoint\n",
    "    _releaseTime : datetime\n",
    "        release time of particles\n",
    "    \"\"\"\n",
    "    def __init__(self, lons, lats, idx = None, releaseTime = None):\n",
    "        assert len(lats) == len(lons), \"lats and lons should be of equal size\"\n",
    "        self._releaseTime = releaseTime\n",
    "        self.lons = lons\n",
    "        self.lats = lats\n",
    "        self.lonlat = np.dstack((lons, lats)) #First axis corresponds to time\n",
    "        # idx should not be updated since this makes triangle points harder to track down\n",
    "        if idx: \n",
    "            self.idx = idx\n",
    "        else:\n",
    "            self.idx = np.arange(self.n)\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self.lonlat.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def releaseTimes(self):\n",
    "        if self._releaseTime:\n",
    "            return [self._releaseTime for part in range(self.n)]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_regular_grid(cls, nlon, nlat, minLat=60., maxLat=90., minLon=-180, maxLon=180, **kwargs):\n",
    "        \"\"\"\n",
    "        Grid construction by dividing latitude and longitude ranges into a discrete amount of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlat : int\n",
    "            number of latitudes.\n",
    "        nlon : int\n",
    "            number of longitudes.\n",
    "        minLat : float\n",
    "            minimum latitude of grid (southern boundary)\n",
    "        maxLat : float\n",
    "            maximum latitude of grid (northern boundary)\n",
    "        minLon : float\n",
    "            minimum longitude of grid (western boundary)\n",
    "        maxLon : float\n",
    "            maximum longitude of grid (eastern boundary)\n",
    "        \"\"\"\n",
    "        lonRange = np.linspace(minLon, maxLon, nlon)\n",
    "        latRange = np.linspace(minLat, maxLat, nlat)\n",
    "        lon2D, lat2D = np.meshgrid(lonRange, latRange)\n",
    "        return cls(lon2D.flatten(), lat2D.flatten(), **kwargs)\n",
    "    \n",
    "    def remove_on_land(self, fieldset):\n",
    "        \"\"\"\n",
    "        Uses the fieldset.landMask to remove particles that are located on land (where u, v == 0 or -1)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fieldset : Parcels.FieldSet\n",
    "            should have a landMask attribute (created by fieldSetter)\n",
    "        \"\"\"\n",
    "        \n",
    "        nBefore = self.n\n",
    "        # Load landmask and initialize mask for particles on land \n",
    "        landMask = fieldset.landMask\n",
    "        try:\n",
    "            landMask = landMask.compute()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        # Use scipy.interpolate.griddata to have particles adopt value of landmask from nearest neighbor\n",
    "        lonlatMask = griddata(np.dstack((fieldset.U.grid.lon.flatten(), \n",
    "                                         fieldset.U.grid.lat.flatten()))[0,:,:], \n",
    "                              landMask.flatten(), \n",
    "                              self.lonlat[0,:,:], \n",
    "                              method='nearest')\n",
    "        self.lonlat = self.lonlat[:, ~lonlatMask, :]\n",
    "        self.lons = self.lonlat[0, :, 0]\n",
    "        self.lats = self.lonlat[0, :, 1]\n",
    "        nAfter = self.n\n",
    "        self.removedParticleCount = nBefore - nAfter\n",
    "    \n",
    "    def add_advected_from_pset(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Add final particle locations by loading them from a pset. See `lonlat_from_pset()`.    \n",
    "        \"\"\"\n",
    "        lonlatFinal = lonlat_from_pset(pset, *args, **kwargs)[1]\n",
    "        self.lonlat = np.concatenate((self.lonlat, lonlat_final), axis=0)\n",
    "    \n",
    "    def show(self, tindex = 0, export = None, projection=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a plot of the particle locations in particles object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tindex : int\n",
    "            Index of lonlat pairs (0 is initial, 1 is final).\n",
    "        export : str\n",
    "            Name of exported figure. A directory 'figures' is created.\n",
    "        \"\"\"\n",
    "        fig = plt.figure()\n",
    "        if projection:\n",
    "            ax = plt.axes(projection = projection)\n",
    "        else:\n",
    "            ax = plt.axes(projection = cart.crs.PlateCarree())\n",
    "        ax.scatter(self.lonlat[tindex, :, 0], self.lonlat[tindex, :, 1], transform = cart.crs.Geodetic(), **kwargs)\n",
    "        ax.add_feature(cart.feature.COASTLINE)\n",
    "        if export:\n",
    "            if not os.path.exists('figures'):\n",
    "                os.makedirs('figures')\n",
    "            if export[-4] == '.':\n",
    "                plt.savefig(f'figures/{export}', dpi=300)\n",
    "            else:\n",
    "                plt.savefig(f'figures/{export}.png', dpi=300)\n",
    "        return ax\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class countBins:\n",
    "    \"\"\"\n",
    "    Bins used for counting particles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    binType : str\n",
    "        Indicates the type of bin: `regular` or `icosahedral`\n",
    "    \"\"\"\n",
    "    def __init__(self, bindex):\n",
    "        self.bindex = bindex\n",
    "        \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.bindex)\n",
    "    \n",
    "    def load_communities(self, comFile, parser = 'clu'):\n",
    "        \"\"\"\n",
    "        Load communities determined by a community detection algorithm on a regular grid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        comFile : str\n",
    "            Filename of community file\n",
    "        parser : str\n",
    "            Parser to use\n",
    "        \"\"\"\n",
    "        #----- START PARSERS -----#\n",
    "        if parser == 'legacy':\n",
    "            self.communityDF = pd.read_csv(comFile,  delimiter=\" \").set_index('node')\n",
    "        if parser == 'clu':\n",
    "            with open(comFile) as cluFile:\n",
    "                clu = cluFile.read().split('\\n')\n",
    "            self.codelength = float(clu[0].split(' ')[3])\n",
    "            header = clu[1].split(' ')[1:]\n",
    "            body = [line.split(' ') for line in clu[2:] if line is not '']\n",
    "            self.communityDF = pd.DataFrame(body, columns=header).astype({\"node\" : 'int', \n",
    "                                                                          \"module\" : 'int', \n",
    "                                                                          \"flow\" : 'float' }).set_index(\"node\")\n",
    "        if parser == 'tree':\n",
    "            \"\"\"\n",
    "            Not yet fully impelemented. Should have the option to investigate multiple tree levels\n",
    "            \"\"\"\n",
    "            with open(comFile) as treeFile:\n",
    "                tree = treeFile.read().split('\\n')\n",
    "            self.codelength = float(tree[0].split(' ')[3])\n",
    "            header = tree[1].split(' ')[1:]\n",
    "            body = [line.split(' ') for line in tree[2:] if line is not '']\n",
    "            self.communityDF = pd.DataFrame(body, columns=header).drop(columns=\"name\").rename(columns={'physicalId': 'node'})\n",
    "            self.communityDF['rank'] = self.communityDF['path'].map(lambda a: a.split(\":\")[-1])\n",
    "            self.communityDF['module'] = self.communityDF['path'].map(lambda a: a.split(\":\")[-2])\n",
    "            self.communityDF = self.communityDF.astype({\"node\" : \"int\",  \"module\" : \"int\", \"flow\" : \"float\"}).set_index(\"node\")\n",
    "        #------ END PARSERS ------#\n",
    "        #2D\n",
    "        communityID = np.zeros(self.n)\n",
    "        communityID.fill(np.nan)\n",
    "        for n in range(self.n):\n",
    "            communityID[n] = int(self.communityDF['module'].loc[self.bindex[n]+1])\n",
    "        self.communityID = communityID\n",
    "        \n",
    "    def color_communities(self, num_colors=4):\n",
    "        \"\"\"Associate new colors to existing communities by using graph coloring.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_colors : int\n",
    "            Number of colors that will be used for coloring the map. Currently, if `num_colors` is less than or \n",
    "            equal to the maximum degree, `num_colors` is increased to maxDegree+1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Array containing new community IDs, corresponding to different colors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.communityNetwork = nx.Graph()\n",
    "            for community in self.adjacencyDict:\n",
    "                for neighbor in self.adjacencyDict[community]:\n",
    "                    self.communityNetwork.add_edge(community, neighbor)\n",
    "            # Remove self-loops\n",
    "            self.communityNetwork.remove_edges_from(self.communityNetwork.selfloop_edges())\n",
    "        except NameError:\n",
    "            raise RuntimeError('The counting grid does not yet have an adjacency dictionary for determining the coloring of communities. Try calling the `find_adjacency()` method first.')\n",
    "        maxDegree = max([d for n, d in self.communityNetwork.degree()])\n",
    "        if not nx.algorithms.planarity.check_planarity(self.communityNetwork)[0]:\n",
    "            print('Graph is not planar!')\n",
    "            if maxDegree >= num_colors:\n",
    "                num_colors = maxDegree+1\n",
    "                print(f'Using {maxDegree+1} colors instead.')\n",
    "        #self.colorMapping = nx.coloring.equitable_color(self.communityNetwork, num_colors=num_colors)\n",
    "        self.colorMapping = nx.coloring.greedy_color(self.communityNetwork, strategy='largest_first')\n",
    "        self.colorID = np.array([self.colorMapping[index] for index in self.communityID.flatten()]).reshape(self.communityID.shape)\n",
    "        return self.colorID\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regularCountBins(countBins):\n",
    "    def __init__(self, nlon, nlat, minLat=60., maxLat=90., minLon=-180, maxLon=180, **kwargs):\n",
    "        \"\"\"\n",
    "        Grid construction by dividing latitude and longitude ranges into a discrete amount of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlat : int\n",
    "            number of latitudes.\n",
    "        nlon : int\n",
    "            number of longitudes.\n",
    "        minLat : float\n",
    "            minimum latitude of grid (southern boundary)\n",
    "        maxLat : float\n",
    "            maximum latitude of grid (northern boundary)\n",
    "        minLon : float\n",
    "            minimum longitude of grid (western boundary)\n",
    "        maxLon : float\n",
    "            maximum longitude of grid (eastern boundary)\n",
    "        \"\"\"\n",
    "        self.binType = 'regular'\n",
    "        dlat = (maxLat - minLat)/nlat\n",
    "        dlon = (maxLon - minLon)/nlon\n",
    "        lonOffset = dlon/2\n",
    "        latOffset = dlat/2\n",
    "        self.lonBounds = np.linspace(minLon, maxLon, nlon+1)\n",
    "        self.latBounds = np.linspace(minLat, maxLat, nlat+1)\n",
    "        lonCenters = np.linspace(minLon + lonOffset, maxLon - lonOffset, nlon)\n",
    "        latCenters = np.linspace(minLat + latOffset, maxLat - latOffset, nlat)\n",
    "        self.lonCenters2D, self.latCenters2D = np.meshgrid(lonCenters, latCenters)\n",
    "        self.lonIdx2D, self.latIdx2D = np.meshgrid(np.arange(nlon), np.arange(nlat))\n",
    "        self.gridShape = self.lonIdx2D.shape\n",
    "        super().__init__(np.arange(len(self.lonIdx2D.flatten())))\n",
    "        self.bindex2D = self.bindex.reshape(self.gridShape)\n",
    "        \n",
    "    def particle_count(self, particles, tindex=0):\n",
    "        count = np.histogram2d(particles.lonlat[tindex,:,0], particles.lonlat[tindex,:,1], bins=[self.lonBounds, self.latBounds])[0]\n",
    "        if tindex == 0:\n",
    "            self.initCount = count\n",
    "        return count\n",
    "    \n",
    "    def find_adjacency(self, mode='Neumann'):\n",
    "        \"\"\"\n",
    "        Create an adjacency list: for each node (grid cell), determine which nodes are bordering this node.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mode : string\n",
    "            Either 'Neumann' or 'Moore'. Indicates the pixel neighborhood used for determining\n",
    "            neighbors. The Von Neumann neighborhood only considers pixels touching the edges to\n",
    "            be neighbors, while the Moore neighborhood also considers pixels touching the \n",
    "            corners.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Containing keys corresponding to community IDs and values being `set` objects\n",
    "            containing IDs of bordering communities.\n",
    "        \"\"\"\n",
    "        assert self.binType == \"regular\", \"Bin type must be regular.\"\n",
    "        # Construct empty adjacency dictionary\n",
    "        # Using dictionary so that labels coincide labels created by InfoMap, rather than being \n",
    "        # indices, which might not coincide with labels.\n",
    "        communityID2D = self.communityID.reshape(self.gridShape)\n",
    "        self.adjacencyDict = {}\n",
    "        # Iterate over all cells\n",
    "        for i in range(self.gridShape[0]):\n",
    "            for j in range(self.gridShape[1]):\n",
    "                # Save current community in variable\n",
    "                currentCommunity = int(communityID2D[i,j])\n",
    "                # If the current community doesn't have a key and value yet, add an empty\n",
    "                # set to the dictionary, with the key being the community ID.\n",
    "                if currentCommunity not in self.adjacencyDict:\n",
    "                    self.adjacencyDict[currentCommunity] = set()\n",
    "                self.adjacencyDict[currentCommunity].add(int(communityID2D[i, j+1//self.gridShape[1]]))\n",
    "                self.adjacencyDict[currentCommunity].add(int(communityID2D[i, j-1]))\n",
    "                # Careful at northern and southern boundaries. \n",
    "                if i<self.gridShape[0]-1:\n",
    "                    self.adjacencyDict[currentCommunity].add(int(communityID2D[i+1, j]))\n",
    "                    if mode == 'Moore':\n",
    "                        self.adjacencyDict[currentCommunity].add(int(communityID2D[i+1, j+1//self.gridShape[1]]))\n",
    "                        self.adjacencyDict[currentCommunity].add(int(communityID2D[i+1, j-1]))\n",
    "                if i>0:\n",
    "                    self.adjacencyDict[currentCommunity].add(int(communityID2D[i-1, j]))\n",
    "                    if mode == 'Moore':\n",
    "                        self.adjacencyDict[currentCommunity].add(int(communityID2D[i-1, j+1//self.gridShape[1]]))\n",
    "                        self.adjacencyDict[currentCommunity].add(int(communityID2D[i-1, j-1]))\n",
    "        return self.adjacencyDict\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transMat:\n",
    "    def __init__(self, counter):\n",
    "        self.counter = counter\n",
    "        self.sums = np.tile(self.counter.sum(axis=1), (self.counter.shape[1],1)).T\n",
    "        self.data = np.divide(self.counter, self.sums, out=np.zeros_like(self.sums), where=self.sums!=0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pset(cls, pset, countBins, timedelta64 = None):\n",
    "        \"\"\"\n",
    "        Create transition matrix from particle trajectories (from `pset`) given a `countBins`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pset : parcels.ParticleSet\n",
    "            Particle set containing particle trajectories.\n",
    "        countBins : comtools.countBins\n",
    "            Grid containing cells on which the transition matrix is to be created.\n",
    "        timedelta64 : np.timedelta64\n",
    "            Timedelta relating to the elapsed time of the particle run for which the transition \n",
    "            matrix is to be determined. Example: np.timedelta64(30,'D') for 30 days.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        comtools.transmat\n",
    "            Transition matrix object, including attributes `counter` containing particle\n",
    "            tranistions, and  `sums` used for normalization.\n",
    "\n",
    "        Issues\n",
    "        ------\n",
    "        lonlats with NaN values will be put at index 60, 30 respectively. \n",
    "        For these we don't want to look up the bindex\n",
    "        \"\"\"\n",
    "        lonlatInit, lonlatFinal = lonlat_from_pset(pset, timedelta64)\n",
    "        # Find initial and final counting bin index for each particle\n",
    "        if countBins.binType == 'regular':\n",
    "            bindexInit = np.dstack((np.searchsorted(countBins.lonBounds, lonlatInit[0,:,0]),\n",
    "                                     np.searchsorted(countBins.latBounds, lonlatInit[0,:,1])))[0]-1\n",
    "            bindexFinal = np.dstack((np.searchsorted(countBins.lonBounds, lonlatFinal[0,:,0]), \n",
    "                                      np.searchsorted(countBins.latBounds, lonlatFinal[0,:,1])))[0]-1\n",
    "        elif countBins.binType == 'icosahedral':\n",
    "            raise NotImplementedError(\"Transition matrices from icosahedral grids still need to be implemented\")\n",
    "        \n",
    "        counter = np.zeros((countBins.n, countBins.n))\n",
    "        N = bindexInit.shape[0]\n",
    "        # Constructing transition matrix from bin indices\n",
    "        for i in range(N):\n",
    "            # Print progress\n",
    "            inProg = np.linspace(0, N, num=100, dtype='int')\n",
    "            if i in inProg or i == N-1:\n",
    "                print (f\"\\r Determining particle bins. {int(np.ceil(i/(N-1)*100))}%\", end=\"\")\n",
    "            # Only applies to regular grid\n",
    "            if countBins.binType == 'regular':\n",
    "                if (    bindexFinal[i,0] < countBins.gridShape[1] - 1\n",
    "                    and bindexFinal[i,0] >= 0\n",
    "                    and bindexFinal[i,1] < countBins.gridShape[0]\n",
    "                    and bindexFinal[i,1] >= 0): # moved outside domain\n",
    "                    sourceIdx = countBins.bindex2D[bindexInit[i,1], bindexInit[i,0]]\n",
    "                    destIdx = countBins.bindex2D[bindexFinal[i,1], bindexFinal[i,0]]\n",
    "                    counter[sourceIdx, destIdx] += 1\n",
    "            elif countBins.binType == 'icosahedral':\n",
    "                raise NotImplementedError(\"Transition matrices from icosahedral grids still need to be implemented\")\n",
    "        return cls(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
