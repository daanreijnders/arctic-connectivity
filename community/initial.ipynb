{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cart\n",
    "\n",
    "from datetime import timedelta as delta\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "from parcels import (grid, Field, FieldSet, ParticleSet, JITParticle, ScipyParticle, AdvectionRK4,\n",
    "                     ErrorCode, ParticleFile, Variable, plotTrajectoriesFile)\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append('/home/students/4302001/arctic-connectivity/tools')\n",
    "sys.path.append('/Users/daanreijnders/surfdrive/Thesis/repo/tools')\n",
    "import plot\n",
    "import lifeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lifeline' from '/home/students/4302001/arctic-connectivity/tools/lifeline.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(plot)\n",
    "reload(lifeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to velocity field and mesh\n",
    "# readdir_ocean = '/Users/daanreijnders/Datasets/'\n",
    "# readdir_ice = '/Users/daanreijnders/Datasets/'\n",
    "# readdir_mesh = '/Users/daanreijnders/Datasets/'\n",
    "\n",
    "readdir_ice = '/data/oceanparcels/input_data/CESM/0.1_deg/control/ice/arctic/'\n",
    "readdir_ocean = '/data/oceanparcels/input_data/CESM/0.1_deg/control/ocean/arctic/'\n",
    "readdir_mesh = '/scratch/DaanR/fields/'\n",
    "\n",
    "fieldfile_ocean = 'daily_CESM_0.1degree_controlrun_year_300_arctic_region_timed.nc'\n",
    "fieldfile_ice = 'monthly_icefields_CESM_0.1degree_controlrun_year_300_arctic.nc'\n",
    "meshfile = 'POP_grid_coordinates.nc'\n",
    "\n",
    "writedir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_velocity_field(fieldfiles, meshfile=None):\n",
    "    if not meshfile:\n",
    "        meshfile = fieldfiles\n",
    "    filenames = {'U': {'lon': meshfile,\n",
    "                       'lat': meshfile,\n",
    "                       'data':fieldfiles},\n",
    "                 'V': {'lon': meshfile,\n",
    "                       'lat': meshfile,\n",
    "                       'data':fieldfiles}}\n",
    "\n",
    "    variables = {'U': 'UVEL_5m',\n",
    "                 'V': 'VVEL_5m'}\n",
    "\n",
    "    dimensions = {'U': {'time': 'time',\n",
    "                        'lat': 'ULAT',\n",
    "                        'lon': 'ULON'},\n",
    "                  'V': {'time': 'time',\n",
    "                        'lat': 'ULAT',\n",
    "                        'lon': 'ULON'}}\n",
    "    fieldset = FieldSet.from_pop(filenames, variables, dimensions, allow_time_extrapolation=False)\n",
    "    fieldset.U.vmax = 10;  fieldset.U.vmin = -10;  # set max of flow to 10 m/s\n",
    "    fieldset.V.vmax = 10; fieldset.V.vmin = -10;\n",
    "    \n",
    "    fieldset.computeTimeChunk(fieldset.U.grid.time[0], 1)\n",
    "    fieldset.landMask = np.logical_or(fieldset.U.data[0,:,:]==-0.01, np.abs(fieldset.U.data[0,:,:])<0.0000001)\n",
    "    return fieldset\n",
    "\n",
    "\n",
    "# def add_ice_fields(fieldset, fieldfile, iceVars=['aice', 'hisnap', 'hi'], meshfile=None):\n",
    "#     \"\"\"Not working for now\"\"\"\n",
    "#     if not meshfile:\n",
    "#         meshfile = fieldfiles\n",
    "#     for varName in iceVars:\n",
    "#         filenames = {'lon': [meshfile],\n",
    "#                      'lat': [meshfile],\n",
    "#                      'data': [fieldfile]}\n",
    "#         variable = (varName, varName)\n",
    "#         dimensions = {'time': 'time',\n",
    "#                       'lat': 'TLAT',\n",
    "#                       'lon': 'TLON'}\n",
    "#         field = Field.from_netcdf(filenames, variable, dimensions, allow_time_extrapolation=False)\n",
    "#         fieldset.add_field(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Casting depth data to np.float32\n"
     ]
    }
   ],
   "source": [
    "fieldset = read_velocity_field(readdir_ocean+fieldfile_ocean, meshfile=readdir_mesh+meshfile)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_ice_fields(fieldset, readdir_ice+fieldfile_ice, meshfile=readdir_mesh+meshfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGrid:\n",
    "    def __init__(self, nlon, nlat, minLat=60., maxLat=90., minLon=-180, maxLon=180):\n",
    "        self.nlat = nlat\n",
    "        self.nlon = nlon\n",
    "        self.dlat = (maxLat - minLat)/nlat\n",
    "        self.dlon = 360/nlon\n",
    "        \n",
    "class countGrid(myGrid):\n",
    "    def __init__(self, nlon, nlat, minLat=60., maxLat=90., minLon=-180, maxLon=180):\n",
    "        myGrid.__init__(self, nlon, nlat, minLat, maxLat, minLon, maxLon)\n",
    "        \n",
    "        self.latOffset = self.dlat/2\n",
    "        self.lonOffset = self.dlon/2\n",
    "        self.latBounds = np.linspace(minLat, maxLat, nlat+1)\n",
    "        self.lonBounds = np.linspace(minLon, maxLon, nlon+1)\n",
    "        latarr = np.linspace(minLat+self.latOffset, maxLat-self.latOffset, nlat) # center\n",
    "        lonarr = np.linspace(minLon+self.lonOffset, maxLon-self.lonOffset, nlon) # center\n",
    "        self.lons, self.lats = np.meshgrid(lonarr, latarr)\n",
    "        self.lonIdx, self.latIdx = np.meshgrid(np.arange(self.nlon), np.arange(self.nlat))\n",
    "        self.flatIdx = np.arange(len(self.lonIdx.flatten())).reshape(self.lonIdx.shape)\n",
    "        \n",
    "    def countInit(self, particleGrid):\n",
    "        self.initCount = np.histogram2d(particleGrid.lonlat[:,0], particleGrid.lonlat[:,1], bins=[self.lonBounds, self.latBounds])\n",
    "\n",
    "    \n",
    "class particleGrid(myGrid):\n",
    "    def __init__(self, nlon, nlat, release_time, minLat=60.5, maxLat=89.5, minLon=-179.5, maxLon=179.5):\n",
    "        myGrid.__init__(self, nlon, nlat, minLat, maxLat, minLon, maxLon)\n",
    "        self.advected = False\n",
    "        if release_time:\n",
    "                self.release_time = release_time\n",
    "        # Create mesh\n",
    "        self.latarr = np.linspace(minLat, maxLat, nlat) # particle position\n",
    "        self.lonarr = np.linspace(minLon, maxLon, nlon) # particle position\n",
    "        self.lons, self.lats = np.meshgrid(self.lonarr, self.latarr)\n",
    "        \n",
    "        # Create pairs and flatten using a reshape\n",
    "        self.lonlat_3d = np.dstack((self.lons, self.lats))\n",
    "        self.initialParticleCount = self.countParticles()\n",
    "        self.lonlat = np.array([np.reshape(self.lonlat_3d, (self.particleCount, 2))])\n",
    "        \n",
    "        # Create labels\n",
    "        self.lonlat_labels = np.arange(self.particleCount)\n",
    "        \n",
    "    def countParticles(self):\n",
    "        if not hasattr(self, 'lonlat') and self.advected == False:\n",
    "            self.particleCount = self.lonlat_3d.shape[0]*self.lonlat_3d.shape[1]\n",
    "        else:\n",
    "            self.particleCount = self.lonlat.shape[1]\n",
    "        return self.particleCount\n",
    "    \n",
    "    def remove_on_land(self, fieldset):\n",
    "        # Load landmask and initialize mask for particles on land \n",
    "        lm = fieldset.landMask\n",
    "        # Use scipy.interpolate.griddata to have particles adopt value of landmask from nearest neighbor\n",
    "        lonlatMask = griddata(np.dstack((fieldset.U.grid.lon.flatten(), fieldset.U.grid.lat.flatten()))[0,:,:], \n",
    "                              lm.flatten(), \n",
    "                              self.lonlat[0,:,:], method='nearest')\n",
    "        self.lonlat = self.lonlat[:, ~lonlatMask, :]\n",
    "        self.removedParticleCount = self.countParticles()\n",
    "        # recreate labels\n",
    "        self.lonlat_labels = np.arange(self.particleCount)\n",
    "        \n",
    "    def add_advected(self, pset):\n",
    "        if hasattr(self, 'removedParticleCount'):\n",
    "            n = self.removedParticleCount\n",
    "        else:\n",
    "            n = self.initialParticleCount\n",
    "        newPos = np.zeros((1, n, 2))\n",
    "        for idx in np.arange(n):\n",
    "             newPos[0, idx, :] = [pset[idx].lon, pset[idx].lat]\n",
    "        self.lonlat = np.concatenate((self.lonlat, newPos), axis=0)\n",
    "\n",
    "    def getBindex(self, countGrid, tindex=0):\n",
    "        bindex = np.dstack((np.searchsorted(countGrid.lonBounds, self.lonlat[tindex,:,0]), np.searchsorted(countGrid.latBounds, self.lonlat[tindex, :,1])))\n",
    "        return bindex\n",
    "# Counter does not actually track positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapLon(particle, fieldset, time):\n",
    "    if particle.lon > 180.:\n",
    "        particle.lon = particle.lon - 360.\n",
    "    if particle.lon < -180.:\n",
    "        particle.lon = particle.lon + 360.\n",
    "\n",
    "def deleteParticle(particle, fieldset, time):\n",
    "    particle.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle execution function\n",
    "def gridAdvection(fieldset, \\\n",
    "                  countGrid, \\\n",
    "                  particleGrid, \\\n",
    "                  experiment_name='', \\\n",
    "                  runtime=delta(days=30), \\\n",
    "                  dt = delta(minutes=5), \\\n",
    "                  outputdt = delta(hours = 12)):\n",
    "    pset = ParticleSet.from_list(fieldset, JITParticle, particleGrid.lonlat[0,:,0], particleGrid.lonlat[0,:,1])\n",
    "    kernels = pset.Kernel(AdvectionRK4) + pset.Kernel(wrapLon)\n",
    "    pfile = pset.ParticleFile(name = writedir+\"pset_\"+experiment_name, outputdt=outputdt)\n",
    "    print(f\"Run: Advecting particles for {runtime}\")\n",
    "    pset.execute(kernels, \\\n",
    "                 runtime = runtime, \\\n",
    "                 dt = dt, \\\n",
    "                 output_file = pfile, \\\n",
    "                 recovery = {ErrorCode.ErrorOutOfBounds: deleteParticle})\n",
    "    return pset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countG = countGrid(60, 30)\n",
    "particleG = particleGrid(119, 59, 0)\n",
    "particleG.remove_on_land(fieldset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: Advecting particles for 30 days, 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled JITParticleAdvectionRK4wrapLon ==> /tmp/parcels-23300/42f29ff6371d311be3f69a426625c387.so\n",
      "100% (2592000.0 of 2592000.0) |##########| Elapsed Time: 0:04:55 Time:  0:04:55\n"
     ]
    }
   ],
   "source": [
    "psetTest = gridAdvection(fieldset, countG, particleG, experiment_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTransition(pset, countGrid, fieldset, timedelta64=None):\n",
    "    ds = xr.open_dataset(pset)\n",
    "    lons = ds['lon'].data\n",
    "    lats = ds['lat'].data\n",
    "    ids = ds['traj'].data\n",
    "    times = ds['time'].data\n",
    "    \n",
    "    if np.any(np.diff(times[:,0]).astype(bool)):\n",
    "        warning.warn(\"Not all starting times are equal. Behaviour may not be as expected.\", Warning)\n",
    "    if timedelta64:\n",
    "        final_tidx = np.searchsorted(times[0,:], times[0,0]+timedelta64)\n",
    "        if final_tidx == times.shape[1]:\n",
    "            warning.warn(\"`final_tidx` lies outside of time window. Choosing last index instead\", Warning)\n",
    "            final_tidx = times.shape[1]-1\n",
    "    else:\n",
    "        final_tidx = times.shape[1]-1\n",
    "    lonlat_init = np.dstack((ds['lon'].data[:,0], ds['lat'].data[:,0]))\n",
    "    lonlat_final = np.dstack((ds['lon'].data[:,final_tidx], ds['lat'].data[:,final_tidx]))\n",
    "    \n",
    "    bindex_init = np.dstack((np.searchsorted(countGrid.lonBounds, lonlat_init[:,0]), np.searchsorted(countGrid.latBounds, lonlat_init[:,1])))\n",
    "    bindex_final = np.dstack((np.searchsorted(countGrid.lonBounds, lonlat_final[:,0]), np.searchsorted(countGrid.latBounds, lonlat_final[:,1])))\n",
    "    ds.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "createTransition('pset_test.nc', countG, fieldset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
